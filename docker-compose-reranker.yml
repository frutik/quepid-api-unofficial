services:
  llamacpp-reranker:
    image: ghcr.io/ggml-org/llama.cpp:server
    platform: linux/amd64
    ports:
      - 8080:8080
    volumes:
      - ./data/llama.cpp/models:/app/models
    environment:
      LLAMA_ARG_MODEL_URL: "https://huggingface.co/dengcao/Qwen3-Reranker-4B-GGUF/resolve/main/Qwen3-Reranker-4B-q4_k_m.gguf"
      LLAMA_ARG_MODEL: "/app/models/Qwen3-Reranker-4B-q4_k_m.gguf"
      LLAMA_ARG_CTX_SIZE: 4096
      LLAMA_ARG_RERANKING: true
      LLAMA_ARG_N_PARALLEL: 2
      LLAMA_ARG_ENDPOINT_METRICS: 1
      LLAMA_ARG_PORT: 8080
#  qwen3-reranker-8b:
#    container_name: qwen3-reranker-8b
#    image: dengcao/vllm-openai:v0.9.2
#    platform: linux/amd64
#    volumes:
#      - ./data/vllm/models:/models
##    command: ['--model', '/models/Qwen3-Reranker-8B',  '--served-model-name', 'Qwen3-Reranker-8B',  '--gpu-memory-utilization', '0.90', '--hf_overrides','{"architectures": ["Qwen3ForSequenceClassification"],"classifier_from_token": ["no", "yes"],"is_original_qwen3_reranker": true}']
#    command: ['--model', '/models/Qwen3-Reranker-8B',  '--served-model-name', 'Qwen3-Reranker-8B',  '--hf_overrides','{"architectures": ["Qwen3ForSequenceClassification"],"classifier_from_token": ["no", "yes"],"is_original_qwen3_reranker": true}']
#    environment:
#      CUDA_VISIBLE_DEVICES: ""
#      VLLM_TARGET_DEVICE: "cpu"
#      VLLM_LOGGING_LEVEL: DEBUG
#    ports:
#      - 8012:8000
